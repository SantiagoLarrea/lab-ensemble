{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB | Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**\n",
    "\n",
    "In this challenge, we will be working with the same Spaceship Titanic data, like the previous Lab. The data can be found here:\n",
    "\n",
    "https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\n",
    "\n",
    "Metadata\n",
    "\n",
    "https://github.com/data-bootcamp-v4/data/blob/main/spaceship_titanic.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Lab, you should try different ensemble methods in order to see if can obtain a better model than before. In order to do a fair comparison, you should perform the same feature scaling, engineering applied in previous Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6606, 24), (6606,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carga de datos\n",
    "spaceship = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\")\n",
    "\n",
    "# Limpieza simple consistente con labs previos\n",
    "spaceship = spaceship.dropna()\n",
    "\n",
    "# FE mínima: quedarnos con la letra de la cabina\n",
    "spaceship['CabinDeck'] = spaceship['Cabin'].str[0].str.upper()\n",
    "\n",
    "# Target y features\n",
    "y = spaceship['Transported']\n",
    "X = pd.get_dummies(\n",
    "    spaceship.drop(columns=['Transported', 'PassengerId', 'Name', 'Cabin']),\n",
    "    drop_first=False\n",
    ")\n",
    "\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the same as before:\n",
    "- Feature Scaling\n",
    "- Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6606, 24)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Escalado\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Selección de características (ajusta k si quieres)\n",
    "k = min(30, X.shape[1])   # p.ej., top-30\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_fs = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "X_fs.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5284, 24), (1322, 24))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_fs, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Selection** - now you will try to apply different ensemble methods in order to get a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging and Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging -> Acc: 0.7806354009077155  F1-macro: 0.7805746336996338\n",
      "Pasting -> Acc: 0.7776096822995462  F1-macro: 0.7775771018891766\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Bagging (bootstrap=True)\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=200,\n",
    "    max_samples=0.8,\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "y_pred_bag = bagging_clf.predict(X_test)\n",
    "results['Bagging'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_bag),\n",
    "    'f1_macro': f1_score(y_test, y_pred_bag, average='macro')\n",
    "}\n",
    "print(\"Bagging -> Acc:\", results['Bagging']['accuracy'], \" F1-macro:\", results['Bagging']['f1_macro'])\n",
    "\n",
    "# Pasting (bootstrap=False)\n",
    "pasting_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=200,\n",
    "    max_samples=0.8,\n",
    "    bootstrap=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "pasting_clf.fit(X_train, y_train)\n",
    "y_pred_paste = pasting_clf.predict(X_test)\n",
    "results['Pasting'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_paste),\n",
    "    'f1_macro': f1_score(y_test, y_pred_paste, average='macro')\n",
    "}\n",
    "print(\"Pasting -> Acc:\", results['Pasting']['accuracy'], \" F1-macro:\", results['Pasting']['f1_macro'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest -> Acc: 0.7813918305597579  F1-macro: 0.781391705475192\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "results['RandomForest'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_rf),\n",
    "    'f1_macro': f1_score(y_test, y_pred_rf, average='macro')\n",
    "}\n",
    "print(\"Random Forest -> Acc:\", results['RandomForest']['accuracy'], \" F1-macro:\", results['RandomForest']['f1_macro'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting -> Acc: 0.783661119515885  F1-macro: 0.7828659770606059\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "gb_clf.fit(X_train, y_train)\n",
    "y_pred_gb = gb_clf.predict(X_test)\n",
    "results['GradientBoosting'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_gb),\n",
    "    'f1_macro': f1_score(y_test, y_pred_gb, average='macro')\n",
    "}\n",
    "print(\"Gradient Boosting -> Acc:\", results['GradientBoosting']['accuracy'], \" F1-macro:\", results['GradientBoosting']['f1_macro'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost -> Acc: 0.7738275340393344  F1-macro: 0.7737186455794826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred_ada = ada_clf.predict(X_test)\n",
    "results['AdaBoost'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_ada),\n",
    "    'f1_macro': f1_score(y_test, y_pred_ada, average='macro')\n",
    "}\n",
    "print(\"AdaBoost -> Acc:\", results['AdaBoost']['accuracy'], \" F1-macro:\", results['AdaBoost']['f1_macro'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is the best and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  accuracy  f1_macro\n",
      "GradientBoosting  0.783661  0.782866\n",
      "RandomForest      0.781392  0.781392\n",
      "Bagging           0.780635  0.780575\n",
      "Pasting           0.777610  0.777577\n",
      "AdaBoost          0.773828  0.773719\n",
      "\n",
      "Mejor modelo: GradientBoosting\n",
      "Accuracy: 0.7837 | F1-macro: 0.7829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nComentario:\\nSeleccionamos el mejor modelo en función de accuracy y F1-macro sobre el conjunto de test.\\n- Bagging/Pasting suelen mejorar modelos base al reducir varianza.\\n- Random Forest aprovecha múltiples árboles con bootstrap + submuestreo de features, robusto y con buen sesgo-varianza.\\n- Gradient Boosting y AdaBoost optimizan secuencialmente corrigiendo errores previos; suelen rendir muy bien si el ruido no es alto.\\nEn nuestra comparación (arriba), el modelo con mejor métrica general es el indicado como 'Mejor modelo'.\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#comment here\n",
    "import pandas as pd\n",
    "\n",
    "# Tabla de resultados\n",
    "score_df = pd.DataFrame(results).T.sort_values(by=['accuracy','f1_macro'], ascending=False)\n",
    "print(score_df)\n",
    "\n",
    "best_model = score_df.index[0]\n",
    "best_acc = score_df.loc[best_model, 'accuracy']\n",
    "best_f1  = score_df.loc[best_model, 'f1_macro']\n",
    "\n",
    "print(\"\\nMejor modelo:\", best_model)\n",
    "print(f\"Accuracy: {best_acc:.4f} | F1-macro: {best_f1:.4f}\")\n",
    "\n",
    "\"\"\"\n",
    "Comentario:\n",
    "Seleccionamos el mejor modelo en función de accuracy y F1-macro sobre el conjunto de test.\n",
    "- Bagging/Pasting suelen mejorar modelos base al reducir varianza.\n",
    "- Random Forest aprovecha múltiples árboles con bootstrap + submuestreo de features, robusto y con buen sesgo-varianza.\n",
    "- Gradient Boosting y AdaBoost optimizan secuencialmente corrigiendo errores previos; suelen rendir muy bien si el ruido no es alto.\n",
    "En nuestra comparación (arriba), el modelo con mejor métrica general es el indicado como 'Mejor modelo'.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
